{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "01AO6HK0u_m4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1YLNtm8gNsviTEnVXzfiby2VMKrc0XzLP\" width=\"500\"/>\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9YehS8enAmDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep learning module introduction**\n",
        "\n",
        "#### **Morning contents/agenda**\n",
        "\n",
        "0. Start at the end\n",
        "\n",
        "1. Description of the module's contents, materials and resources:\n",
        "  - github repository\n",
        "  - schedule and assessments\n",
        "  - teaching team\n",
        "\n",
        "2. Google Colab introduction\n",
        "\n",
        "3. Deep learning?\n",
        "\n",
        "4. Why `PyTorch`?\n",
        "\n",
        "5. A simple feed-forward network (FFN) with `PyTorch`\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Overview the module contents and how they will be delivered\n",
        "\n",
        "2. Understand the assessment process for the module\n",
        "\n",
        "3. First contact with `PyTorch` and FFNs\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "#### **Afternoon contents/agenda**\n",
        "\n",
        "1. https://playground.tensorflow.org\n",
        "\n",
        "2. Improving our simpleFFN\n",
        "\n",
        "#### **Learning outcomes**\n",
        "\n",
        "1. Understand the effect of different network configurations with tensorflow\n",
        "\n",
        "2. Break the morning example and raise questions about how `PyTorch` works.\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "J-VVpU1MJJOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Start at the end\n",
        "\n",
        "I want to start this module by addressing three basic questions:\n",
        "- **Why** are we doing this module?\n",
        "- **How** is that useful for your career?\n",
        "- **What** are we going to learn?\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "Let's see what the state-of-the-art in deep learning looks like. We will see a couple more examples on state-of-the-art image generation later today, but for now let's focus on what everybody knows about:\n",
        "\n",
        "[chatGPT 4](https://chat.openai.com/) \\\\\n",
        "[Google Gemini](https://bard.google.com/chat)\n",
        "\n",
        "Let's ask these models some relatively simple questions:\n",
        "\n",
        "- *Write a sentence with 10 words where every word starts with the same letter as the previous word finishes*\n",
        "- *If I have three eggs, and two needles, and a laptop, how can I build the tallest tower that is stable?*\n",
        "\n",
        "**How did these models do?** By the end of the module, we will have seen what are the building blocks that make this type of networks (and others) perform as well as they do.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "GnB_qwdgkKhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Description of module's contents\n",
        "\n",
        "Information about the module can be found in this github repo:\n",
        "\n",
        "https://github.com/ese-ada-lovelace-2024/DL_Module_2024\n",
        "\n",
        "The repository contains all the teaching materials (which will be released as the module progresses), dates for assessments, primer material, bibliography, and other useful information.\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "sinIebTtMUvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Google Colab\n",
        "\n",
        "During the module we will use [Google Colab](https://colab.research.google.com/).\n",
        "\n",
        "You will need to have access to a Google Account.\n",
        "\n",
        "#### **I strongly suggest that you make a dedicated google account for this module**\n",
        "\n",
        "During the first break of the morning, you will create a new google account that you will only use for this module. Do not reuse your existing google accounts because we will be providing you with the means to activate a Colab Pro pay-as-you-go license on Wednesday 4th December to provide you additional compute capabilities for the courseworks. Until then, you can use the free version of Colab.\n",
        "\n",
        "\n",
        "**WEDNESDAY 4 DECEMBER @ 11:30H WE WILL SET UP OUR COLAB PRO ACCOUNTS IN CLASS**\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Mounting your google drive**\n",
        "\n",
        "You can use google drive to store and access files e.g. storing and loading data from numpy or CSV files.\n",
        "Use the following command to mount your GDrive and access your files:\n"
      ],
      "metadata": {
        "id": "yojhzj6cLxp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "P6Y5eHvr1BzL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d389d78-3723-4dac-9285-b8f2c61d210f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using the GPU**\n",
        "\n",
        "Google Colab enables us to obtain free GPU computing resources.  \n",
        "You can switch the runtime of Google Colab from CPU to GPU based via the ```toolbar```:  \n",
        "\n",
        "Commands:\n",
        "```Runtime -> Change Runtime Type -> Hardware Accelerator -> GPU```"
      ],
      "metadata": {
        "id": "iUF5NF_12ILl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.device_count() > 0 and torch.cuda.is_available():\n",
        "    print(\"Cuda installed! Running on GPU!\")\n",
        "    device = 'cuda'\n",
        "else:\n",
        "    print(\"No GPU available!\")\n",
        "    device = 'cpu'"
      ],
      "metadata": {
        "id": "x4G2qi-w2NNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657df101-68ed-4076-92ec-84348f308a21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuda installed! Running on GPU!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But when using the free version of Colab, you will only have access to one GPU at a time. You can change your runtime type by:\n",
        "\n",
        "```Runtime -> Change Runtime Type -> Hardware Accelerator -> CPU```\n",
        "\n",
        "If you are not actively training a network, or even if the training is not very demanding computationally, you can switch to CPUs."
      ],
      "metadata": {
        "id": "AM-bEJZcNiQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Using Git Repositories**\n",
        "\n",
        "You can clone Git repositories as well!\n",
        "(You can even commit to Git Repositories via ```File -> Save a copy in Github```)\n",
        "Use the ```%cd [directory]``` jupyter cell-magic to access the folder."
      ],
      "metadata": {
        "id": "3ZykFEOu5mYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/josephmisiti/awesome-machine-learning\n",
        "%cd awesome-machine-learning"
      ],
      "metadata": {
        "id": "_PPrKK3R5sE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd89fc94-cdb7-4612-d0bb-af74efe550dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'awesome-machine-learning'...\n",
            "remote: Enumerating objects: 4874, done.\u001b[K\n",
            "remote: Counting objects: 100% (442/442), done.\u001b[K\n",
            "remote: Compressing objects: 100% (271/271), done.\u001b[K\n",
            "remote: Total 4874 (delta 251), reused 338 (delta 170), pack-reused 4432 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4874/4874), 2.48 MiB | 10.02 MiB/s, done.\n",
            "Resolving deltas: 100% (2535/2535), done.\n",
            "/content/awesome-machine-learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!du -sh"
      ],
      "metadata": {
        "id": "W9vWaGtD5u9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1169e7-23c0-44ee-964f-4405c8022818"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blogs.md  courses.md  LICENSE\t  ml-curriculum.md  scripts\n",
            "books.md  events.md   meetups.md  README.md\n",
            "3.1M\t.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving and Storing Colab Notebooks Locally**\n",
        "\n",
        "Google Colab stores notebooks automatically but you can also save and store your work manually:\n",
        "\n",
        "```File -> Save ```\n",
        "\n",
        "or download the .ipynb file:\n",
        "\n",
        "```File -> Download .ipynb ```\n",
        "\n",
        "<br/>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>"
      ],
      "metadata": {
        "id": "42H-yBXG5x_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Deep Learning?\n",
        "\n",
        "### **What is deep learning?**\n",
        "\n",
        "\n",
        "[From Wikipedia](https://en.wikipedia.org/wiki/Deep_learning):\n",
        "\n",
        "*Deep learning is a class of machine learning algorithms that[8]: 199–200  uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.*\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/b/bb/AI-ML-DL.svg\" width=\"400\"/></center>\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "### **Why do we need deep learning?**\n",
        "\n",
        "The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) (in the case of arbitrary width) states that a single-layer network with sufficient neurons is a universal approximator for univariate functions, but not for multivariate functions. If we want networks to capture more complex data distributions, then we need to introduce more complexity by adding layers (i.e., increase the depth of the network).\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/9/99/Neural_network_example.svg\" width=\"200\"/></center>\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "### **Neurons: the basic building blocks of deep learning**\n",
        "\n",
        "We have been talking about neurons, layers, and networks as the basic building blocks of deep learning. But, what do these concepts mean?\n",
        "\n",
        "What we call neurons in deep learning is loosely inspired by how biological neurons work:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1vk5DzUD7Y8iQvvXmYvdBOC1Ih43XzWUy\" width=\"500\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "In simplified terms, biological neurons receive multiple inputs (through their dendrites), combine them together (within the cell body), and then apply a non-linear threshold to determine whether to become activated or remain deactivated. If activated, the neuron then sends an output inpulse (through its axon).\n",
        "\n",
        "Similarly, our artificial neurons receive multiple inputs (each corresponding to a single numerical value), combine them using some linear function (e.g. a weighted sum in feed-forward neural networks or a convolution in convolutional neural networks), and apply a non-linear **activation function** to the result. The result of this operation is then passed on as the neuron's output:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1MmBR_zW_peAZjsyHLJGTWLs9WRICdcOX\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "The linear operation that neurons perform is not fixed by design. Instead, it depends on some weights, which we will define formally later, that we call **parameters**. These parameters are fine-tuned through a process that we call **training**.\n",
        "\n",
        "Individual neurons are stacked together to form layers, and layers are concatenated to form networks:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1eXIdCncuBhYzc50qRheu1pWd49Iu-Q_O\" width=\"500\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\\\\[1cm]$$\n",
        "\n",
        "### **Origins of AI**\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1STE1tkh98Lrm64ukF6Yxi6squyxcOQRl\" width=\"800\"/>\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "\n",
        "### **Applications of ML and DL**\n",
        "\n",
        "- **Finance**: predict patterns at short and long scales.\n",
        "\n",
        "- **Health**: triage and diagnostic, image segmentation, automated data handling, and a long etc.\n",
        "\n",
        "- **Retail**: offer personal recommendations based on recent choices of products.\n",
        "\n",
        "- **Marketing**: organize customers into classes based on their past behavior.\n",
        "\n",
        "- **Automotive**: self-driving vehicles, safety technology, etc.\n",
        "\n",
        "- **Media**: generate images of non-existing people for synthetic scenes.\n",
        "\n",
        "- **Art**: synthetic generation of music, text or paintings mimicking styles of well known artists,\n",
        "\n",
        "- …and **Science**, of course.\n",
        "\n",
        "<br>\n",
        "\n",
        "Modern networks perform complex tasks at levels that a few years ago would seem like science-fiction:\n",
        "\n",
        "<br>\n",
        "\n",
        "- GPT-3 \\[175 billion parameters\\] & GPT-4 \\[1.7 trillion parameters\\]\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1cN5IIz6nRNQVySwywm7YcEGh5DOH39Dh\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- Alphafold \\[21 million parameters\\]\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1u9yqERCWyRQ6uZzlfXPC-IT6jPNK9vkC\" width=\"800\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- [DALL·E 2](https://labs.openai.com/) \\[3.5 billion parameters\\] & [DALL·E 3](https://openai.com/dall-e-3) \\[?? parameters\\]:\n",
        "\n",
        "\n",
        "DALL·E 2 | DALL·E 3\n",
        "- | -\n",
        "<img src=\"https://drive.google.com/uc?id=1fyiaaWidUeUhXD738V01MT7IaFejURD2\" width=\"500\"/>| <img src=\"https://drive.google.com/uc?id=1Cyj8UKTjabb-UyBWZJmjJUi87ur1BN3k\" width=\"500\"/>\n",
        "\n",
        "<center>origami sculpture of a knight holding his shield while fighting a fire breathing dragon, slight depth of field </center>\n",
        "\n",
        "<br>\n",
        "\n",
        "DALL·E 2 | DALL·E 3\n",
        "- | -\n",
        "<img src=\"https://drive.google.com/uc?id=1nNpJ-QIhdPbZvGTHlMHFvLLTMCsWnC4w\" width=\"500\"/> | <img src=\"https://drive.google.com/uc?id=1m75eEO2oY8UMX2vCyLxuNIAPKdlbCM7l\" width=\"500\"/>\n",
        "\n",
        "<center>a class of deep learning students in the style of classic roman painting</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "another DALL·E 3 output: \\\\\n",
        "(how do you think the prompt was changed to generate this one?)\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Dm4VrgQ02cKGHmAKqHz4OoLYJCQxzfDc\" width=\"500\"/>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "DALL·E 2 | DALL·E 3\n",
        "- | -\n",
        "<img src=\"https://drive.google.com/uc?id=1c_F9BRPPmUgB4q8qfW4lvIBNP8ZWk8qw\" width=\"500\"/> | <img src=\"https://drive.google.com/uc?id=1-sDOLM_AOVrZI0A5Muc-2FBoF-qbbxNE\" width=\"500\"/>\n",
        "\n",
        "<center>highest possible stacked tower using four eggs, a book, two rugby balls, and a pack of cards</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "- ...and many others that we will discuss during the module.\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "### **Godfathers of deep learning**\n",
        "\n",
        "Perhaps the three most important contributors in the field are Yoshua Bengio, Yann LeCunn, and Geoffrey Hinton (Nobel Prize in Physics 2024!):\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1Y1S2GUZUK67_JSFaCDqknTcRief97Ije\" width=\"500\"/></center>\n",
        "\n",
        "- Turing Award [video](https://www.youtube.com/watch?v=HzilDIhWhrE)\n",
        "\n",
        "- AAAI (Association for the Advancement of Artificial Intelligence) award [video](https://www.youtube.com/watch?v=UX8OubxsY8w)\n",
        "\n",
        "They are involved in the development of a significant number of concepts we will learn over the next three weeks.\n",
        "\n",
        "$$\\\\[2cm]$$\n",
        "\n",
        "### **The data explosion behind machine learning**\n",
        "\n",
        "Machine learning, and deep learning in particular, rely on having access to very large amounts of data. Over the last decade the world has experienced an exponential growth of data creation.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1dW45nnliVyJ-rqRM1af_0iChsc-iY87i\" width=\"800\"/>\n",
        "\n",
        "\n",
        "`1 zettabyte = 10^21 bytes`\n",
        "\n",
        "Such volumes **demand automated solutions** to extract and use existing patterns in the data.\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n"
      ],
      "metadata": {
        "id": "0cH6S_tA00Ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Why `PyTorch`?\n",
        "\n",
        "First of all, why Python at all? According to the [Kaggle survey](https://www.kaggle.com/kaggle-survey-2022), Python is the most popular programming language for data science and machine learning:\n",
        "\n",
        "<center><img src=\"https://lh5.googleusercontent.com/0eRpLN5qo-YGtM9lb3_8obDkl4w8O82EE0QBVLV2SSwrZMSMj4UJyKgAhVr2UMp4wP3M29BPoCm-KMGpZRl_9qqFgkS3uzWHCuY6mbdPqPNTQ-gQY_Ve24nIHv9bSbaBk4msWAG4l6KumlYT3aMNzcRW5Cr2QTBBPTLxLF8F7yZco4m1L6Q0sTC9hhEH9enyCKQ\" width=\"700\"/></center>\n",
        "\n",
        "`PyTorch` tends to be the preferred framework in academic research environments, whereas `Tensorflow` is mostly used in production and deployment. According to the same survey, `PyTorch` is still in its growing phase, whereas `TensorFlow/Keras` is in slight decline:\n",
        "\n",
        "<center><img src=\"https://lh4.googleusercontent.com/m-r1FHxaE0olotAgkbK4H3csXjsZ5KhDYIHJlvmSTTATg1LmdpiAcGPslrnwXM06M6eJU1mOQtaxC6lrIuXsrFSMvJP06j9_EaF2DUsuVLMeZo2WLmhfq5_tizRlxgBnBESPrkK2MRsDa9oy2vy6rlf_ydWiMPNf0ZNJuuF6BtZfVXBrGxyFG1EUnbc9Z_pAgKM\" width=\"700\"/></center>\n",
        "\n",
        "But don't read too much into these numbers, at the end of the day, the choice of `PyTorch` is arbitrary and responds to the personal preferences of the teaching team, and the fact that we are a research institution.\n",
        "\n",
        "Finally, Google Colab is the most widely used cloud notebook solution, where the trend indicates that it will likely dominate the market in the coming years:\n",
        "\n",
        "<center><img src=\"https://lh4.googleusercontent.com/Ax-MHWndntRG1_FJKOez0UkpfgklOOFldSv6euggJC1UoGR_9xMs-5vIiivjR88XBR-iLhc_5q3Xo5IRzc_ZOonYRIx5TUnZ9o-Go3SMICwh1ylGstwCY4HIzS4pX08X_XB2BuBQGHP9nR2-0_eg6IRuSujvo3fYQeClDnHnX7KYvjlAwRvlipI-l14jEeLXKM8\" width=\"700\"/></center>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br/>\n",
        "\n"
      ],
      "metadata": {
        "id": "JZt3YanrMixQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. A simple feed-forward network (FFN) with `PyTorch`\n",
        "\n",
        "We will devote the rest of the morning to implement a rather shallow feed forward network using `PyTorch`.\n",
        "\n",
        "But first, let's clarify a few concepts.\n",
        "\n",
        "### **What is a feed-forward network?**\n",
        "\n",
        "In essence, is this:"
      ],
      "metadata": {
        "id": "SmQOPX6M_Fpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%html\n",
        "<iframe src=\"https://www.3blue1brown.com/lessons/neural-networks\" width=\"1200\" height=\"700\"></iframe>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "RG2LAPUe_GdM",
        "outputId": "186eeb4e-f9e9-46c5-d10f-ce336aec3cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://www.3blue1brown.com/lessons/neural-networks\" width=\"1200\" height=\"700\"></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given some input, the network performs operations on its elements (pixels in this case, but does not have to be!) a few times (through layers), and then outputs some data at the end (in this case, the probability of the input to be a particular hand-written digit).\n",
        "\n",
        "But what are these components that we represent as circles in the network? They are the neurons of the network, which we have represented before like this:\n",
        "\n",
        "<br>\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?id=1AtQhgLLuGKiLi8neVLNo1xQ91rfW7XLj\" width=\"800\"/></center>\n",
        "\n",
        "\n",
        "and can be converted to mathematical operations like this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1rAh2U6ejO54rptSXbHTYqiTYg78mvOTe\" width=\"800\"/>\n",
        "\n",
        "But so far, we have only seen how a network performs what is called a **forward pass**\n",
        "\n",
        "\\[***SIDE NOTE***:\n",
        "*Due to the field being relatively young and covering a very wide range of applications, nomenclature in DL (and ML in general) is far from consistent. We will see that $\\theta$ values are often referred as $w$ and $b$ (for $w$eights and $b$iases), for example.*\\]\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5jqC6YvI8PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters $\\theta_{ij}^{(k)}$ of the network control how well the network performs a task (assigns a digit to its correct class in this particular example). As you can see, the network in the gif above already knows what it's doing, given that the last layer provides the largest probability to the correct class.\n",
        "\n",
        "But when we attack a new problem, we only have a dataset and the intention to use a neural network to solve the problem. We will not talk today about how to choose the best network architecture (that is, how many hidden layers, how many neurons per layer, what activation functions to use, etc.), as this will be covered over the course of the module.\n",
        "\n",
        "Let's assume for now that we have decided to use a particular network architecture. This network is untrained, and by that we mean that since it has never seen the data, we have no idea of what the network parameters should be. The common approach is then, to assign random numbers to these parameters and see what happens.\n",
        "\n",
        "This happens:\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://www.3blue1brown.com/content/lessons/2017/gradient-descent/random-trash.png\" width=800>\n",
        "\n",
        "<br>\n",
        "\n",
        "So the question now is how do we improve these parameters so that the network does what we want (which is to correctly classify my digits in this case)? The answer is to **train the network**.\n",
        "\n",
        "Training means that we will use known information about our data (its inherent structure, or some labels associated to it, for example) to modify the parameters $\\theta$ of our network. We do this by defining a **loss function** that captures the accuracy of the predictions of our network.\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1mztVlVpRmWk5_mjH3wERPI-YGkX-47pY\" alt=\"Gradient descent, how neural networks learn | Chapter 2, Deep learning\" width=\"800\">\n",
        "\n",
        "<br>\n",
        "\n",
        "We use local optimisation techniques (based on the gradient-descent method and its numerous variations) to train our networks. Why do we do this instead of exploring all the possible combinations of parameters and picking the one that performs best? *Remember how many parameters where there in some of the networks above?*\n",
        "\n",
        "The **loss function** defines a hypersurface in a multi-dimensional space that has as many dimensions as there are network parameters. This hypersurface is known as the [solution space, the feasible region, the search space or the feasible set](https://en.wikipedia.org/wiki/Feasible_region).\n",
        "\n",
        "The goal of training is to find the lowest value in this solution space (or at least a sufficiently low local minimum). In a very simple case with two dimensions (two network parameters) we will do this:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=13y_aSC-9QBHBXJr0CvQzDRCxyxoPQtJP\" alt=\"optimisation\" width=\"500\"/>\n",
        "\n",
        "<br>\n",
        "\n",
        "And in order to achieve this we will use **backpropagation**.\n",
        "\n",
        "**Backpropagation** is a method that will let us improve our network parameters by iteratively updating them in a direction that reduces the magnitude of the loss.\n",
        "\n",
        "And now, it (hopefully) has become clearer why we use local optimisation: we can afford to compute the value of the loss for a particular set of $\\theta$ values, as well as the gradient of the loss with respect to these $\\theta$ values using backpropagation. But using a global method (like Monte Carlo optimisation) would be unaffordable because that would require to run forward passes of the network a ridiculously large number of times (remember we are optimising a loss that depends on many parameters, or dimensions).\n",
        "\n",
        "Coming back to why we use `PyTorch` (or any other framework really): **we use it to avoid having to manually code the backpropagation algorithm for every network that we use.**\n"
      ],
      "metadata": {
        "id": "_hHwWXAHvm71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Testing a simple FFN with `PyTorch`**\n",
        "\n",
        "\\[***NOTE***:\n",
        "*Wednesday's lecture will be dedicated to understanding how to use `PyTorch`. In the example below you can skip the code definition for now, don't worry if it does not quite make sense yet. You can come back to revisit the code after we have introduced `PyTorch`.*\\]"
      ],
      "metadata": {
        "id": "bJ9XSxeh4sAW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code definition - Skip to Network training for now"
      ],
      "metadata": {
        "id": "01AO6HK0u_m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycm livelossplot\n",
        "%pylab inline\n",
        "\n",
        "# Let's start by importing some Python and `Pytorch` libraries and define a random seed to be able to replicate our results.\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from livelossplot import PlotLosses\n",
        "from pycm import *\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torchsummary import summary\n",
        "\n",
        "def set_seed(seed):\n",
        "    \"\"\"\n",
        "    Use this to set ALL the random seeds to a fixed value and take out any randomness from cuda kernels\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    torch.backends.cudnn.benchmark = False  # uses the inbuilt cudnn auto-tuner to find the fastest convolution algorithms. Useful when inputs do not change size -\n",
        "    torch.backends.cudnn.enabled   = False\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op6Hlylx62Is",
        "outputId": "779e95cd-403b-4df4-800a-358ce3dcfa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycm\n",
            "  Downloading pycm-4.1-py3-none-any.whl.metadata (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting art>=1.8 (from pycm)\n",
            "  Downloading art-6.3-py3-none-any.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pycm) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.7.1)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.4.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.3.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (24.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2024.9.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Downloading pycm-4.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Downloading art-6.3-py3-none-any.whl (606 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.3/606.3 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: art, pycm, livelossplot\n",
            "Successfully installed art-6.3 livelossplot-0.5.5 pycm-4.1\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a class called `simpleFFN` and we test that it reads an input and gives us an output:"
      ],
      "metadata": {
        "id": "V2eAOGGX5S4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class simpleFFN(nn.Module):\n",
        "  def __init__(self, num_hidden=(200, 50), bias=False, activation=nn.Sigmoid):\n",
        "    super(simpleFFN, self).__init__()\n",
        "\n",
        "    hidden_input = 784\n",
        "    self.len_hidden = len(num_hidden)\n",
        "    for idx, n in enumerate(num_hidden):\n",
        "        setattr(self, 'hidden_%d' % idx, nn.Linear(hidden_input, n, bias=bias))\n",
        "        hidden_input = n\n",
        "    self.output = nn.Linear(hidden_input, 10, bias=bias)\n",
        "\n",
        "    self.activation = activation()\n",
        "\n",
        "  def forward(self, x):\n",
        "    for idx in range(self.len_hidden):\n",
        "        hidden = getattr(self, 'hidden_%d' % idx)\n",
        "        z = hidden(x)\n",
        "        x = self.activation(z)\n",
        "    z = self.output(x)\n",
        "    o = self.activation(z)\n",
        "    return o\n",
        "\n",
        "x = torch.randn((1, 1, 784))\n",
        "model_test = simpleFFN()\n",
        "y = model_test(x)\n",
        "print(y)\n",
        "print(model_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A-yiQg-5GCn",
        "outputId": "35c04bf7-960e-4205-a806-c30800c12eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.5151, 0.4548, 0.5858, 0.5020, 0.4929, 0.5102, 0.4703, 0.5050,\n",
            "          0.5041, 0.5393]]], grad_fn=<SigmoidBackward0>)\n",
            "simpleFFN(\n",
            "  (hidden_0): Linear(in_features=784, out_features=200, bias=False)\n",
            "  (hidden_1): Linear(in_features=200, out_features=50, bias=False)\n",
            "  (output): Linear(in_features=50, out_features=10, bias=False)\n",
            "  (activation): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download the MNIST dataset:"
      ],
      "metadata": {
        "id": "4Qt65hTz7DyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = MNIST(\"./\", download=True, train=True)\n",
        "mnist_test = MNIST(\"./\", download=True, train=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySHnndP76TU3",
        "outputId": "ea14a35d-8139-44ab-ad7a-c9bee59b3e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.36MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 497kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.49MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 3.93MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data in training, validation, and test sets:"
      ],
      "metadata": {
        "id": "CnvweAsQ7RXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data\n",
        "shuffler = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42).split(mnist_train.train_data, mnist_train.train_labels)\n",
        "indices = [(train_idx, validation_idx) for train_idx, validation_idx in shuffler][0]"
      ],
      "metadata": {
        "id": "9hoIxS-w7K_W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ed6be1-8700-49d5-d1cd-cde06b83938f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We standardise the data.\n",
        "\n",
        "\\[***NOTE***:\n",
        "*As we will see in the coming weeks, even though this step is not strictly necessary, it does result in better performance because we are assuming that our data has some underlying Gaussian distribution (the errors rather). In general, it is good practice to standardise our data as it improves convergence speed and results in better generalisation.*\\]"
      ],
      "metadata": {
        "id": "oANi3uz_7hSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_standardization(X): # define an standardisation function\n",
        "  X /= 255.\n",
        "  X -= 0.1307\n",
        "  X /= 0.3081\n",
        "  return X\n",
        "\n",
        "# standardise the data\n",
        "X_train, y_train = apply_standardization(mnist_train.train_data[indices[0]].float()), mnist_train.train_labels[indices[0]]\n",
        "X_val, y_val = apply_standardization(mnist_train.train_data[indices[1]].float()), mnist_train.train_labels[indices[1]]\n",
        "X_test, y_test =  apply_standardization(mnist_test.test_data.float()), mnist_test.test_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7QwS-eY7YwK",
        "outputId": "31fcb36f-4e73-49b6-980b-3741fbf7fa2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:76: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:66: UserWarning: train_labels has been renamed targets\n",
            "  warnings.warn(\"train_labels has been renamed targets\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:81: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:71: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create TensorDatasets (more on this on the next two days):"
      ],
      "metadata": {
        "id": "9lhH-My87o58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the TensorDatasets containing mnist_train, mnist_validate, and mnist_test\n",
        "mnist_train = TensorDataset(X_train, y_train.long())\n",
        "mnist_validate = TensorDataset(X_val, y_val.long())\n",
        "mnist_test = TensorDataset(X_test, y_test.long())"
      ],
      "metadata": {
        "id": "GGdgGj1b7cRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we plot one of the elements of the training set (to check we loaded the data correctly)"
      ],
      "metadata": {
        "id": "9HgxDyZm70VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[11], cmap = 'gray')\n",
        "print(X_train.mean(), X_train.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "nWkgu9Dk7oNX",
        "outputId": "bd3d5e35-c6d1-41de-c851-add64db9f080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0001) tensor(1.0003)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are going to define functions to train and validate our network:"
      ],
      "metadata": {
        "id": "NcYexe918c2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, criterion, data_loader):\n",
        "    model.train()                         # the model is in the training mode so the parameters(weights)to be optimised will be updated\n",
        "    train_loss, train_accuracy = 0, 0     # initialise loss and accuracy to 0 for training\n",
        "    for X, y in data_loader:              # iterate over the mini-batches defined in the data loader\n",
        "        X, y = X.to(device), y.to(device) # send data to the device (GPU in our case)\n",
        "        optimizer.zero_grad()             # resetting optimiser info\n",
        "        a2 = model(X.view(-1, 28*28))     # forward pass\n",
        "        loss = criterion(a2, y)           # compute loss\n",
        "        loss.backward()                   # backpropagation to calculate the gradients\n",
        "        train_loss += loss*X.size(0)      # # add it up for different mini-batches and undo loss normalisation\n",
        "        y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n",
        "        train_accuracy += accuracy_score(y.cpu().numpy(), y_pred.detach().cpu().numpy())*X.size(0) # compute accuracy\n",
        "        optimizer.step()                  # perform a step of gradient descent\n",
        "\n",
        "    return train_loss/len(data_loader.dataset), train_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset\n",
        "\n",
        "\n",
        "def validate(model, criterion, data_loader):      # does not need optimiser\n",
        "    model.eval()                                  # model is set to evaluation mode so no dropout or any other funny stuff here\n",
        "    validation_loss, validation_accuracy = 0., 0. # initialise loss and accuracy to 0 for training\n",
        "    for X, y in data_loader:                      # iterate over the mini-batches defined in the data loader\n",
        "        with torch.no_grad():                     # deactivates autograd engine\n",
        "            X, y = X.to(device), y.to(device)     # send data to the device (GPU in our case)\n",
        "            a2 = model(X.view(-1, 28*28))         # forward pass\n",
        "            loss = criterion(a2, y)               # evaluate loss\n",
        "            validation_loss += loss*X.size(0)     # add it up for different mini-batches and undo loss normalisation\n",
        "            y_pred = F.log_softmax(a2, dim=1).max(1)[1]  # get predictions\n",
        "            validation_accuracy += accuracy_score(y.cpu().numpy(), y_pred.cpu().numpy())*X.size(0) # compute accuracy\n",
        "\n",
        "    return validation_loss/len(data_loader.dataset), validation_accuracy/len(data_loader.dataset)  # here we can average over the whole dataset"
      ],
      "metadata": {
        "id": "CgYj5Wa37x_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to figure out how good it is, we need to implement an evaluate function:"
      ],
      "metadata": {
        "id": "0gdEKoBYx_5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    ys, y_preds = [], []\n",
        "    for X, y in data_loader:\n",
        "        with torch.no_grad():\n",
        "            X, y = X.to(device), y.to(device) # data and labels to device\n",
        "            a2 = model(X.view(-1, 28*28))     # forward pass and reshape tensor and get it ready to the fully connected layer\n",
        "            y_pred = F.log_softmax(a2, dim=1).max(1)[1] # calculate prediction\n",
        "            ys.append(y.cpu().numpy())        # save predictions\n",
        "            y_preds.append(y_pred.cpu().numpy()) # save predictions\n",
        "\n",
        "    return np.concatenate(y_preds, 0),  np.concatenate(ys, 0) ## concatenate the labels of each batch into a single list"
      ],
      "metadata": {
        "id": "uRRutgRTyBBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also create a function to instantiate our model and send it to the device:"
      ],
      "metadata": {
        "id": "bh4_llah1szt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(*args, **kwargs):\n",
        "    model = simpleFFN(*args, **kwargs).to(device)                                              # instantiate model and send it to the GPU\n",
        "    return model"
      ],
      "metadata": {
        "id": "eKTpCD0A1xi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we are ready to train our network:"
      ],
      "metadata": {
        "id": "kIVOfj46_F8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(model, seed, lr, momentum, batch_size, test_batch_size, n_epochs):\n",
        "    set_seed(seed)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)   # instantiate the optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=0) ## num_workers=0 means that the main process will retrieve the data.\n",
        "    validation_loader = DataLoader(mnist_validate, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
        "    test_loader = DataLoader(mnist_test, batch_size=test_batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    liveloss = PlotLosses()    # plots evolution of loss and accuracy\n",
        "    for epoch in range(n_epochs):\n",
        "        logs = {}\n",
        "        train_loss, train_accuracy = train(model, optimizer, criterion, train_loader)\n",
        "\n",
        "        logs['' + 'log loss'] = train_loss.item()\n",
        "        logs['' + 'accuracy'] = train_accuracy\n",
        "\n",
        "        validation_loss, validation_accuracy = validate(model, criterion, validation_loader)\n",
        "        logs['val_' + 'log loss'] = validation_loss.item()\n",
        "        logs['val_' + 'accuracy'] = validation_accuracy\n",
        "\n",
        "        liveloss.update(logs)\n",
        "        liveloss.draw()\n",
        "        print(validation_loss.item())\n",
        "\n",
        "    return train_loader, validation_loader, test_loader"
      ],
      "metadata": {
        "id": "u35NfsVj9veJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network training\n",
        "\n",
        "Select the hyperparameters of our training (we will learn what each of these does during our second lecture):"
      ],
      "metadata": {
        "id": "6APeIM-9xpe1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = ## ## keep at that if you want to get almost the exact same results (down to numerical precision I guess)\n",
        "lr = ##\n",
        "momentum = ##\n",
        "batch_size = ##\n",
        "test_batch_size = ##\n",
        "n_epochs = ##"
      ],
      "metadata": {
        "id": "72ZUlHzUxxvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the hyperparameters of our network:"
      ],
      "metadata": {
        "id": "BmX5f92p3Qsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_hidden = ##   # number of neurons in each hidden layer - in this case, we use 2 hidden layers: hidden_0 with 200 neurons and hidden_1 with 50 neurons\n",
        "bias = ##         # no bias\n",
        "activation = ##   # sigmoid activation function"
      ],
      "metadata": {
        "id": "KXWjvaO53TPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our network model:"
      ],
      "metadata": {
        "id": "7reAyl_a3hjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ##\n",
        "print(model)"
      ],
      "metadata": {
        "id": "xFTi_Pg63k9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we are ready to train our network and display the evolution in real time:"
      ],
      "metadata": {
        "id": "TzLooDgox1Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, validation_loader, test_loader = ##"
      ],
      "metadata": {
        "id": "DqL0kT-kx5ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Voila**, we have now trained our network.\n",
        "\n",
        "In order to figure out how good it is, let's `evaluate` it:"
      ],
      "metadata": {
        "id": "2Qrhqehu-JJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, y_gt = ##\n",
        "print(y_pred, y_gt)"
      ],
      "metadata": {
        "id": "7mWEWhJK94WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to calculate the confusion matrix of the results we just generated, and plot it:"
      ],
      "metadata": {
        "id": "lJMh3-UHABY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = ## # Create the confusion matrix from Data. ConfusionMatrix is a class derived from the pycm library\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ujY_I1M8e_QW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,6))\n",
        "##\n",
        "##"
      ],
      "metadata": {
        "id": "heNRgOLjdyjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "8wdE2obf4qMN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RMZiet5pIGGT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}